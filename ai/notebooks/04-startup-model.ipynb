{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ==================== IMPORTS ====================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==================== CONFIGURATION INITIALE ====================\n",
    "# Pour la reproductibilité\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration du device (GPU si disponible)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "id": "3f4250d4253e2a75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==================== PRÉTRAITEMENT DES DONNÉES ====================\n",
    "def tokenizer_func(text):\n",
    "    \"\"\"Tokeniseur personnalisé pour les noms d'entreprises\"\"\"\n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def build_vocab(texts, specials=['<unk>', '<pad>']):\n",
    "    \"\"\"Construction du vocabulaire à partir des textes\"\"\"\n",
    "    vocab = {token: i for i, token in enumerate(specials)}\n",
    "    counter = Counter(token for text in texts for token in tokenizer_func(text))\n",
    "    for token, _ in counter.most_common():\n",
    "        if token not in vocab:\n",
    "            vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "class CompanyDataset(Dataset):\n",
    "    \"\"\"Dataset personnalisé pour les noms d'entreprises\"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, vocab=None, max_length=50):\n",
    "        self.data = dataframe\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Construction du vocabulaire si non fourni\n",
    "        if vocab is None:\n",
    "            tokenized_texts = [tokenizer_func(text) for text in self.data['company_name']]\n",
    "            self.vocab = build_vocab(self.data['company_name'])\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "\n",
    "        self.pad_idx = self.vocab['<pad>']\n",
    "        self.unk_idx = self.vocab['<unk>']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['company_name']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "\n",
    "        # Tokenisation et conversion numérique\n",
    "        tokens = tokenizer_func(text)\n",
    "        numericalized = [self.vocab.get(token, self.unk_idx) for token in tokens]\n",
    "\n",
    "        # Padding/truncature\n",
    "        if len(numericalized) > self.max_length:\n",
    "            numericalized = numericalized[:self.max_length]\n",
    "        else:\n",
    "            numericalized = numericalized + [self.pad_idx] * (self.max_length - len(numericalized))\n",
    "\n",
    "        return torch.tensor(numericalized, dtype=torch.long), torch.tensor(label, dtype=torch.float)\n"
   ],
   "id": "9fc61fd12623dc82"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==================== CHARGEMENT DES DONNÉES ====================\n",
    "# Chargement du dataset\n",
    "df = pd.read_csv('datasets/company-dataset/dataset_entreprises (2).csv')\n",
    "\n",
    "# Split des données (70% train, 15% val, 15% test)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Création des datasets et dataloaders\n",
    "train_dataset = CompanyDataset(train_df)\n",
    "vocab = train_dataset.vocab\n",
    "vocab_size = len(vocab)\n",
    "max_length = train_dataset.max_length\n",
    "\n",
    "val_dataset = CompanyDataset(val_df, vocab=vocab)\n",
    "test_dataset = CompanyDataset(test_df, vocab=vocab)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ],
   "id": "41dc7c1c652eb521"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==================== ARCHITECTURE DU MODÈLE ====================\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Mécanisme d'attention pour le modèle\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calcul des poids d'attention\n",
    "        attention_weights = F.softmax(self.attention(x), dim=1)\n",
    "        # Application des poids aux features\n",
    "        weighted = torch.sum(attention_weights * x, dim=1)\n",
    "        return weighted, attention_weights.squeeze(-1)\n",
    "\n",
    "class CompanyClassifier(nn.Module):\n",
    "    \"\"\"Modèle complet de classification d'entreprises\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=128, num_layers=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        # Couche d'embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Couche LSTM bidirectionnelle\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers>1 else 0\n",
    "        )\n",
    "\n",
    "        # Mécanisme d'attention\n",
    "        self.attention = Attention(hidden_dim * 2)  # *2 pour bidirectional\n",
    "\n",
    "        # Classificateur\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        # Initialisation des poids\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialisation des poids du modèle\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                if 'lstm' in name:\n",
    "                    # Initialisation orthogonale pour les poids du LSTM\n",
    "                    for i in range(0, param.shape[0], self.lstm.hidden_size):\n",
    "                        nn.init.orthogonal_(param[i:i+self.lstm.hidden_size])\n",
    "                else:\n",
    "                    nn.init.xavier_normal_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding des tokens\n",
    "        embedded = self.embedding_dropout(self.embedding(x))\n",
    "\n",
    "        # Passage dans le LSTM\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "\n",
    "        # Application de l'attention\n",
    "        attn_out, attn_weights = self.attention(lstm_out)\n",
    "\n",
    "        # Classification finale\n",
    "        out = self.dropout(F.relu(self.fc1(attn_out)))\n",
    "        return torch.sigmoid(self.fc2(out)).squeeze(-1), attn_weights\n"
   ],
   "id": "38deb5b062c0c2a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==================== INITIALISATION DU MODÈLE ====================\n",
    "model = CompanyClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=128,\n",
    "    num_layers=1,\n",
    "    dropout=0.5\n",
    ").to(device)\n"
   ],
   "id": "bdf87880975a928"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==================== CONFIGURATION DE L'ENTRAÎNEMENT ====================\n",
    "criterion = nn.BCELoss()  # Fonction de perte\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-4)  # Optimiseur\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)  # Scheduler\n",
    "\n",
    "# Early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n"
   ],
   "id": "c812972fdc80f726"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==================== FONCTIONS D'ÉVALUATION ====================\n",
    "def evaluate(model, loader, criterion):\n",
    "    \"\"\"Évaluation du modèle sur un loader donné\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs, _ = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n"
   ],
   "id": "3418872a184c73f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==================== ENTRAÎNEMENT DU MODÈLE ====================\n",
    "num_epochs = 30\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    # Phase d'entraînement\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "\n",
    "    train_loss = epoch_train_loss / len(train_loader)\n",
    "    train_acc = train_correct / train_total\n",
    "\n",
    "    # Phase de validation\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "\n",
    "    # Enregistrement des métriques\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    # Mise à jour du scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n"
   ],
   "id": "de5612c3a2a3f88c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==================== ÉVALUATION FINALE ====================\n",
    "# Chargement du meilleur modèle\n",
    "model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "\n",
    "# Évaluation sur le test set\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "print(f\"\\nPerformance finale sur le test set:\")\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
    "\n"
   ],
   "id": "3f7a8819b003240f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==================== FONCTION D'INFÉRENCE ====================\n",
    "def predict_company_type(company_name, model, vocab, max_length=50, device='cpu'):\n",
    "    \"\"\"Prédit si une entreprise est une startup ou une grande entreprise\"\"\"\n",
    "\n",
    "    # Prétraitement du texte\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', company_name.lower())\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Tokenisation et conversion numérique\n",
    "    tokens = tokenizer_func(text)\n",
    "    numericalized = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "\n",
    "    # Padding/truncature\n",
    "    if len(numericalized) > max_length:\n",
    "        numericalized = numericalized[:max_length]\n",
    "    else:\n",
    "        numericalized = numericalized + [vocab['<pad>']] * (max_length - len(numericalized))\n",
    "\n",
    "    # Conversion en tensor\n",
    "    input_tensor = torch.tensor([numericalized], dtype=torch.long).to(device)\n",
    "\n",
    "    # Prédiction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output, attn_weights = model(input_tensor)\n",
    "        prob = output.item()\n",
    "        prediction = 1 if prob > 0.5 else 0\n",
    "\n",
    "    # Récupération des poids d'attention\n",
    "    tokens = tokens[:max_length] + ['<pad>'] * (max_length - len(tokens))\n",
    "    attention = attn_weights[0].cpu().numpy()\n",
    "\n",
    "    return {\n",
    "        'prediction': prediction,\n",
    "        'probability': prob,\n",
    "        'tokens': tokens,\n",
    "        'attention': attention\n",
    "    }\n"
   ],
   "id": "45798d142b975c22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==================== SAUVEGARDE DU MODÈLE ====================\n",
    "save_data = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'vocab': vocab,\n",
    "    'max_length': max_length,\n",
    "    'tokenizer_func': tokenizer_func\n",
    "}\n",
    "\n",
    "torch.save(save_data, 'models/company_classifier.pth')"
   ],
   "id": "af1cc03d02f83b14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==================== EXEMPLE D'UTILISATION ====================\n",
    "example_company = \"google\"\n",
    "result = predict_company_type(example_company, model, vocab, max_length, device)\n",
    "\n",
    "print(f\"\\nExemple de prédiction pour '{example_company}':\")\n",
    "print(f\"Classe prédite: {'Startup' if result['prediction'] == 1 else 'Grande entreprise'}\")\n",
    "print(f\"Probabilité: {result['probability']:.4f}\")\n",
    "\n",
    "# Affichage des tokens les plus importants\n",
    "print(\"\\nTokens les plus importants (par poids d'attention):\")\n",
    "token_weights = sorted(zip(result['tokens'], result['attention']), key=lambda x: x[1], reverse=True)[:5]\n",
    "for token, weight in token_weights:\n",
    "    print(f\"{token}: {weight:.4f}\")"
   ],
   "id": "46902539274ca2c1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
