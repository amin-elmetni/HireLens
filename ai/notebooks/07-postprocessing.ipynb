{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from tqdm import tqdm  # Pour la barre de progression\n",
    "from typing import List, Dict, Set, Any  # Pour le typage\n",
    "import logging\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration MongoDB\n",
    "MONGO_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'port': 27017,\n",
    "    'database': 'PFE',\n",
    "    'collection': 'resumes'\n",
    "}"
   ],
   "id": "f982a4cb321d2db2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "class SkillNormalizer:\n",
    "    \"\"\"Classe pour normaliser et regrouper les compétences similaires\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client = MongoClient(MONGO_CONFIG['host'], MONGO_CONFIG['port'])\n",
    "        self.db = self.client[MONGO_CONFIG['database']]\n",
    "        self.collection = self.db[MONGO_CONFIG['collection']]\n",
    "\n",
    "        # Charger le modèle d'embedding (modèle plus performant pour la production)\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "        # Cache pour les compétences déjà traitées\n",
    "        self.skill_cache = {}\n",
    "\n",
    "        # Expressions régulières pour le nettoyage\n",
    "        self.clean_patterns = [\n",
    "            (re.compile(r'\\(.*?\\)'), ''),  # Supprimer les textes entre parenthèses\n",
    "            (re.compile(r'[^a-zA-Z0-9\\s]'), ' '),  # Remplacer les caractères spéciaux\n",
    "            (re.compile(r'\\s+'), ' ')  # Supprimer les espaces multiples\n",
    "        ]\n",
    "\n",
    "    def get_all_skills(self) -> List[str]:\n",
    "        \"\"\"Récupère toutes les compétences uniques de la collection\"\"\"\n",
    "        skills = set()\n",
    "        try:\n",
    "            for doc in self.collection.find({}, {\"skills.name\": 1}):\n",
    "                for skill in doc.get(\"skills\", []):\n",
    "                    if skill.get(\"name\"):\n",
    "                        cleaned = self._preprocess_skill(skill[\"name\"])\n",
    "                        if cleaned:\n",
    "                            skills.add(cleaned)\n",
    "            return list(skills)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de la récupération des compétences: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def _preprocess_skill(self, skill_name: str) -> str:\n",
    "        \"\"\"Prétraitement d'une compétence avant clustering\"\"\"\n",
    "        if not skill_name:\n",
    "            return \"\"\n",
    "\n",
    "        # Nettoyage de base\n",
    "        cleaned = skill_name.lower().strip()\n",
    "\n",
    "        # Appliquer les regex de nettoyage\n",
    "        for pattern, repl in self.clean_patterns:\n",
    "            cleaned = pattern.sub(repl, cleaned)\n",
    "\n",
    "        return cleaned.strip()\n",
    "\n",
    "    def cluster_skills(self, skills: List[str]) -> Dict[str, str]:\n",
    "        \"\"\"Clusterise les compétences similaires en utilisant des embeddings\"\"\"\n",
    "        if not skills:\n",
    "            return {}\n",
    "\n",
    "        logger.info(\"Génération des embeddings pour le clustering...\")\n",
    "        embeddings = self.model.encode(skills, convert_to_tensor=True)\n",
    "\n",
    "        logger.info(\"Clustering des compétences...\")\n",
    "        clustering = DBSCAN(eps=0.4, min_samples=1, metric='cosine').fit(embeddings.cpu())\n",
    "\n",
    "        # Créer des groupes de compétences similaires\n",
    "        clusters = defaultdict(list)\n",
    "        for idx, label in enumerate(clustering.labels_):\n",
    "            clusters[label].append(skills[idx])\n",
    "\n",
    "        # Stratégie de sélection du représentant améliorée\n",
    "        skill_mapping = {}\n",
    "        for cluster in clusters.values():\n",
    "            # On choisit le terme le plus courant ou le plus court\n",
    "            representative = min(cluster, key=lambda x: (len(x.split()), len(x)))\n",
    "            for skill in cluster:\n",
    "                skill_mapping[skill] = representative\n",
    "\n",
    "        return skill_mapping\n",
    "\n",
    "    def process_document(self, doc: Dict[str, Any], skill_mapping: Dict[str, str]) -> Dict[str, Any]:\n",
    "        \"\"\"Traite un document avec le mapping dynamique\"\"\"\n",
    "        try:\n",
    "            # Traitement des compétences\n",
    "            if 'skills' in doc:\n",
    "                cleaned_skills = []\n",
    "                skill_scores = defaultdict(float)\n",
    "\n",
    "                for skill in doc['skills']:\n",
    "                    if not skill.get('name'):\n",
    "                        continue\n",
    "\n",
    "                    cleaned_name = self._preprocess_skill(skill['name'])\n",
    "                    if not cleaned_name:\n",
    "                        continue\n",
    "\n",
    "                    # Utiliser le mapping ou garder le nom original si non mappé\n",
    "                    normalized_name = skill_mapping.get(cleaned_name, cleaned_name).title()\n",
    "\n",
    "                    # Garder le score le plus élevé pour chaque compétence normalisée\n",
    "                    skill_scores[normalized_name] = max(\n",
    "                        skill_scores[normalized_name],\n",
    "                        float(skill.get('score', 0))\n",
    "                    )\n",
    "\n",
    "                # Recréer la liste des compétences\n",
    "                doc['skills'] = [\n",
    "                    {'name': name, 'score': score}\n",
    "                    for name, score in skill_scores.items()\n",
    "                    if name  # Exclure les noms vides\n",
    "                ]\n",
    "\n",
    "\n",
    "            # Conversion des années d'expérience en entier\n",
    "            if 'years_of_experience' in doc:\n",
    "                try:\n",
    "                    # Si c'est déjà un nombre, on le convertit en int directement\n",
    "                    if isinstance(doc['years_of_experience'], (int, float)):\n",
    "                        doc['years_of_experience'] = int(doc['years_of_experience'])\n",
    "                    # Si c'est une chaîne, on essaie d'extraire le nombre\n",
    "                    elif isinstance(doc['years_of_experience'], str):\n",
    "                        # Extraire les chiffres de la chaîne\n",
    "                        years_str = re.search(r'\\d+', doc['years_of_experience'])\n",
    "                        if years_str:\n",
    "                            doc['years_of_experience'] = int(years_str.group())\n",
    "                        else:\n",
    "                            doc['years_of_experience'] = 0\n",
    "                except (ValueError, TypeError) as e:\n",
    "                    logger.warning(f\"Impossible de convertir years_of_experience pour le document {doc.get('_id')}: {str(e)}\")\n",
    "                    doc['years_of_experience'] = 0\n",
    "\n",
    "            return doc\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du document {doc.get('_id')}: {str(e)}\")\n",
    "            return doc\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Exécute le processus complet de normalisation\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Début du processus de normalisation des compétences...\")\n",
    "\n",
    "            # Étape 1: Récupérer et clusteriser les compétences\n",
    "            all_skills = self.get_all_skills()\n",
    "            logger.info(f\"Nombre de compétences uniques avant clustering: {len(all_skills)}\")\n",
    "\n",
    "            skill_mapping = self.cluster_skills(all_skills)\n",
    "            unique_skills = set(skill_mapping.values())\n",
    "            logger.info(f\"Nombre de compétences uniques après clustering: {len(unique_skills)}\")\n",
    "\n",
    "            # Étape 2: Traiter tous les documents\n",
    "            total_docs = self.collection.count_documents({})\n",
    "            logger.info(f\"Traitement de {total_docs} documents...\")\n",
    "\n",
    "            for doc in tqdm(self.collection.find(), total=total_docs):\n",
    "                try:\n",
    "                    cleaned_doc = self.process_document(doc, skill_mapping)\n",
    "\n",
    "                    # Mettre à jour le document\n",
    "                    update_data = {\n",
    "                        'skills': cleaned_doc.get('skills', [])\n",
    "                    }\n",
    "\n",
    "                    # Ajouter years_of_experience seulement s'il est présent dans le doc nettoyé\n",
    "                    if 'years_of_experience' in cleaned_doc:\n",
    "                        update_data['years_of_experience'] = cleaned_doc['years_of_experience']\n",
    "\n",
    "                    self.collection.update_one(\n",
    "                        {'_id': doc['_id']},\n",
    "                        {'$set': update_data}\n",
    "                    )\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Erreur avec le document {doc.get('_id')}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            logger.info(\"Nettoyage intelligent terminé avec succès!\")\n",
    "\n",
    "        finally:\n",
    "            self.client.close()\n"
   ],
   "id": "260a2d58d36d3535"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:31:31,076 - INFO - Use pytorch device_name: cpu\n",
      "2025-06-18 12:31:31,078 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-06-18 12:31:33,027 - INFO - Début du processus de normalisation des compétences...\n",
      "2025-06-18 12:31:33,037 - INFO - Nombre de compétences uniques avant clustering: 230\n",
      "2025-06-18 12:31:33,039 - INFO - Génération des embeddings pour le clustering...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e7f3a65bcbe34f339497f6c599591e8f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:31:33,947 - INFO - Clustering des compétences...\n",
      "2025-06-18 12:31:33,959 - INFO - Nombre de compétences uniques après clustering: 230\n",
      "2025-06-18 12:31:33,963 - INFO - Traitement de 43 documents...\n",
      "100%|██████████| 43/43 [00:00<00:00, 207.73it/s]\n",
      "2025-06-18 12:31:34,184 - INFO - Nettoyage intelligent terminé avec succès!\n"
     ]
    }
   ],
   "execution_count": 6,
   "source": [
    "if __name__ == \"__main__\":\n",
    "    normalizer = SkillNormalizer()\n",
    "    normalizer.run()"
   ],
   "id": "e19344275b6c9a03"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
