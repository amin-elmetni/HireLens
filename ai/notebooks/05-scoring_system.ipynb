{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Importations et Configuration Initiale",
   "id": "9fe73c269b4834ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:12:02.527899Z",
     "start_time": "2025-06-04T12:11:46.737219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch.serialization\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "import joblib\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import logging\n",
    "from typing import List, Union\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration du logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ],
   "id": "43fe9fcfb2f86033",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 2. Classes et Fonctions Utilitaires\n"
   ],
   "id": "cfab7a1122eb7f7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:12:02.565090Z",
     "start_time": "2025-06-04T12:12:02.549929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Encoder personnalisé pour les types numpy\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (np.float32, np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, (np.int32, np.int64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super().default(obj)\n",
    "\n",
    "def convert_numpy_types(data):\n",
    "    \"\"\"Convertit récursivement les types numpy en types Python natifs\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        return {k: convert_numpy_types(v) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [convert_numpy_types(item) for item in data]\n",
    "    elif isinstance(data, (np.float32, np.float64)):\n",
    "        return float(data)\n",
    "    elif isinstance(data, (np.int32, np.int64)):\n",
    "        return int(data)\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        return data.tolist()\n",
    "    return data\n"
   ],
   "id": "e603d0606befa582",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Connexion à MongoDB",
   "id": "50539bdd07236b1a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:12:03.041088Z",
     "start_time": "2025-06-04T12:12:02.691087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def connect_mongodb():\n",
    "    try:\n",
    "        client = MongoClient(\n",
    "            \"mongodb://localhost:27017/\",\n",
    "            serverSelectionTimeoutMS=2000,\n",
    "            connectTimeoutMS=10000,\n",
    "            socketTimeoutMS=10000\n",
    "        )\n",
    "        client.admin.command('ping')\n",
    "        db = client[\"PFE\"]\n",
    "        return db[\"resumes\"]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"ERREUR MongoDB: {str(e)}\")\n",
    "        raise RuntimeError(\"Impossible de se connecter à MongoDB\") from e\n",
    "\n",
    "# Initialisation MongoDB\n",
    "try:\n",
    "    cvs_collection = connect_mongodb()\n",
    "    cvs_data = list(cvs_collection.find({}))\n",
    "    logger.info(f\"{len(cvs_data)} CVs chargés depuis MongoDB\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erreur d'initialisation: {e}\")\n",
    "    cvs_data = []"
   ],
   "id": "7116f52ed17f07dc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:44 CVs chargés depuis MongoDB\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Prédiction de Catégorie avec BERT",
   "id": "68c8f9341c94bc05"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:12:03.160093Z",
     "start_time": "2025-06-04T12:12:03.095094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CVCategoryPredictor:\n",
    "    def __init__(self, model_path: str = None):\n",
    "        \"\"\"Initialise le prédicteur de catégories de CV.\n",
    "\n",
    "        Args:\n",
    "            model_path: Chemin vers le modèle pré-entraîné (dossier contenant les fichiers)\n",
    "                      Si None, utilise le chemin par défaut dans le dossier models/\n",
    "        \"\"\"\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = None\n",
    "        self.mlb = None\n",
    "        self.tokenizer = None\n",
    "        self.threshold = 0.3\n",
    "\n",
    "        # Chemins absolus par défaut\n",
    "        BASE_MODEL_PATH = r\"C:\\Users\\AzComputer\\PyCharmMiscProject\\models\"\n",
    "        self.default_model_path = os.path.join(BASE_MODEL_PATH, \"mon_modele_bert_multilabel\")\n",
    "        self.default_binarizer_path = os.path.join(BASE_MODEL_PATH, \"multilabel_binarizer.pkl\")\n",
    "\n",
    "        # Liste des chemins possibles à essayer\n",
    "        possible_paths = [\n",
    "            self.default_model_path,  # Chemin absolu principal\n",
    "            os.path.join(os.getcwd(), \"models\", \"mon_modele_bert_multilabel\"),  # Relatif au répertoire courant\n",
    "            os.path.join(\"..\", \"models\", \"mon_modele_bert_multilabel\")  # Un niveau au-dessus\n",
    "        ]\n",
    "\n",
    "        if model_path is not None:\n",
    "            possible_paths.insert(0, model_path)  # Ajouter le chemin spécifié en premier\n",
    "\n",
    "        # Trouver le premier chemin valide\n",
    "        self.model_path = None\n",
    "        for path in possible_paths:\n",
    "            if os.path.exists(path):\n",
    "                self.model_path = path\n",
    "                break\n",
    "\n",
    "        if self.model_path is None:\n",
    "            logger.error(f\"Aucun modèle trouvé aux emplacements:\\n\" + \"\\n\".join(possible_paths))\n",
    "            self._load_fallback_model()\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            # 1. Charger le binariseur\n",
    "            binarizer_path = os.path.join(os.path.dirname(self.model_path), \"multilabel_binarizer.pkl\")\n",
    "            self._load_label_binarizer(binarizer_path)\n",
    "\n",
    "            # 2. Charger le modèle principal\n",
    "            logger.info(f\"Chargement du modèle depuis {self.model_path}\")\n",
    "\n",
    "            self.config = AutoConfig.from_pretrained(self.model_path)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                self.model_path,\n",
    "                config=self.config\n",
    "            ).to(self.device)\n",
    "            self.model.eval()\n",
    "\n",
    "            logger.info(\"Modèle principal chargé avec succès\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du chargement du modèle principal: {e}\")\n",
    "            self._load_fallback_model()\n",
    "\n",
    "    def _load_label_binarizer(self, path: str):\n",
    "        \"\"\"Charge le binariseur de labels avec gestion robuste.\"\"\"\n",
    "        try:\n",
    "            with open(path, 'rb') as f:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "                    self.mlb = joblib.load(f)\n",
    "\n",
    "            if not hasattr(self.mlb, 'classes_') or len(self.mlb.classes_) == 0:\n",
    "                raise ValueError(\"Binarizer chargé mais sans classes définies\")\n",
    "\n",
    "            logger.info(f\"Binarizer chargé. Classes disponibles: {self.mlb.classes_}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du chargement du binarizer: {e}\")\n",
    "            self._create_fallback_binarizer()\n",
    "    def _create_fallback_binarizer(self):\n",
    "        \"\"\"Crée un binariseur de secours avec une classe UNKNOWN.\"\"\"\n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.mlb.fit([['UNKNOWN']])\n",
    "        logger.warning(\"Binarizer de secours créé avec la classe UNKNOWN\")\n",
    "\n",
    "    def _load_fallback_model(self):\n",
    "        \"\"\"Charge un modèle de repli en cas d'échec.\"\"\"\n",
    "        try:\n",
    "            logger.warning(\"Chargement du modèle de repli (bert-base-uncased)\")\n",
    "\n",
    "            # Charger le tokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "            # Configuration de base pour le modèle de repli\n",
    "            self.config = AutoConfig.from_pretrained('bert-base-uncased')\n",
    "            self.config.num_labels = 1  # Une seule classe pour UNKNOWN\n",
    "\n",
    "            # Charger le modèle avec from_config pour éviter les poids non initialisés\n",
    "            self.model = AutoModelForSequenceClassification.from_config(self.config).to(self.device)\n",
    "            self.model.eval()\n",
    "\n",
    "            if self.mlb is None:\n",
    "                self._create_fallback_binarizer()\n",
    "\n",
    "            logger.info(\"Modèle de repli chargé avec succès\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Échec critique du chargement du modèle de repli: {e}\")\n",
    "            raise RuntimeError(\"Impossible de charger un modèle de repli\")\n",
    "\n",
    "    def predict_category(self, cv_data: dict) -> List[str]:\n",
    "        \"\"\"Prédit les catégories d'un CV.\n",
    "\n",
    "        Args:\n",
    "            cv_data: Données du CV sous forme de dictionnaire\n",
    "\n",
    "        Returns:\n",
    "            Liste des catégories prédites (ou ['UNKNOWN'] en cas d'erreur)\n",
    "        \"\"\"\n",
    "        if self.model is None or self.mlb is None:\n",
    "            logger.warning(\"Modèle non disponible - retour par défaut\")\n",
    "            return [\"UNKNOWN\"]\n",
    "\n",
    "        try:\n",
    "            # 1. Convertir le CV en texte\n",
    "            text = self._cv_to_text(cv_data)\n",
    "            if not text.strip():\n",
    "                return [\"UNKNOWN\"]\n",
    "\n",
    "            logger.debug(f\"Texte généré pour la prédiction:\\n{text[:500]}...\")\n",
    "\n",
    "            # 2. Tokenization (simplifiée pour le modèle de repli)\n",
    "            inputs = self.tokenizer(\n",
    "                text,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=512,\n",
    "                return_overflowing_tokens=False  # Retiré car cause des problèmes avec BERT\n",
    "            )\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "            # 3. Prédiction\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "\n",
    "            # 4. Post-traitement\n",
    "            logits = outputs.logits\n",
    "            probs = torch.sigmoid(logits)\n",
    "            logger.debug(f\"Probabilités brutes: {probs.cpu().numpy()}\")\n",
    "\n",
    "            # Appliquer le seuil\n",
    "            predictions = (probs > self.threshold).int().cpu().numpy()\n",
    "\n",
    "            # Convertir en labels\n",
    "            predicted_labels = self.mlb.inverse_transform(predictions)\n",
    "            result = list(predicted_labels[0]) if len(predicted_labels[0]) > 0 else [\"UNKNOWN\"]\n",
    "\n",
    "            logger.info(f\"Catégories prédites: {result}\")\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de la prédiction: {str(e)}\", exc_info=True)\n",
    "            return [\"UNKNOWN\"]\n",
    "\n",
    "    def _cv_to_text(self, cv_data: dict) -> str:\n",
    "        \"\"\"Convertit les données structurées du CV en texte pour l'analyse.\n",
    "\n",
    "        Args:\n",
    "            cv_data: Données du CV sous forme de dictionnaire\n",
    "\n",
    "        Returns:\n",
    "            Texte formaté pour l'analyse\n",
    "        \"\"\"\n",
    "        sections = []\n",
    "\n",
    "        # Informations de base\n",
    "        if cv_data.get('name'):\n",
    "            sections.append(f\"CANDIDAT: {cv_data['name']}\")\n",
    "\n",
    "        # Compétences\n",
    "        if cv_data.get('skills'):\n",
    "            skills = [str(s).strip() for s in cv_data['skills'] if s and str(s).strip()]\n",
    "            if skills:\n",
    "                sections.append(\"COMPÉTENCES: \" + ', '.join(skills))\n",
    "\n",
    "        # Expériences professionnelles\n",
    "        experiences = cv_data.get('experiences', [])\n",
    "        exp_texts = []\n",
    "        for exp in experiences:\n",
    "            parts = []\n",
    "            if exp.get('job_title'):\n",
    "                parts.append(f\"Poste: {exp['job_title']}\")\n",
    "            if exp.get('company'):\n",
    "                parts.append(f\"Entreprise: {exp['company']}\")\n",
    "            if exp.get('duration'):\n",
    "                parts.append(f\"Durée: {exp['duration']}\")\n",
    "            if exp.get('description'):\n",
    "                desc = str(exp['description']).replace('\\n', ' ').strip()\n",
    "                parts.append(f\"Description: {desc}\")\n",
    "            if parts:\n",
    "                exp_texts.append(' | '.join(parts))\n",
    "        if exp_texts:\n",
    "            sections.append(\"EXPÉRIENCES PROFESSIONNELLES: \" + ' || '.join(exp_texts))\n",
    "\n",
    "        # Formation\n",
    "        education = cv_data.get('education', [])\n",
    "        edu_texts = []\n",
    "        for edu in education:\n",
    "            parts = []\n",
    "            if edu.get('degree'):\n",
    "                parts.append(f\"Diplôme: {edu['degree']}\")\n",
    "            if edu.get('institution'):\n",
    "                parts.append(f\"Établissement: {edu['institution']}\")\n",
    "            if edu.get('year'):\n",
    "                parts.append(f\"Année: {edu['year']}\")\n",
    "            if parts:\n",
    "                edu_texts.append(' - '.join(parts))\n",
    "        if edu_texts:\n",
    "            sections.append(\"FORMATION: \" + ' | '.join(edu_texts))\n",
    "\n",
    "        # Projets\n",
    "        projects = cv_data.get('projects', [])\n",
    "        if projects:\n",
    "            proj_texts = []\n",
    "            for proj in projects:\n",
    "                parts = []\n",
    "                if proj.get('project_title'):\n",
    "                    parts.append(f\"Titre: {proj['project_title']}\")\n",
    "                if proj.get('project_summary'):\n",
    "                    summary = str(proj['project_summary']).replace('\\n', ' ').strip()\n",
    "                    parts.append(f\"Résumé: {summary}\")\n",
    "                if parts:\n",
    "                    proj_texts.append(' | '.join(parts))\n",
    "            if proj_texts:\n",
    "                sections.append(\"PROJETS: \" + ' || '.join(proj_texts))\n",
    "\n",
    "        # Certifications\n",
    "        if cv_data.get('certifications'):\n",
    "            certs = [str(c).strip() for c in cv_data['certifications'] if c and str(c).strip()]\n",
    "            if certs:\n",
    "                sections.append(\"CERTIFICATIONS: \" + ', '.join(certs))\n",
    "\n",
    "        return '\\n\\n'.join(sections).strip()\n",
    "\n",
    "    def set_threshold(self, threshold: float):\n",
    "        \"\"\"Définit le seuil de probabilité pour les prédictions.\n",
    "\n",
    "        Args:\n",
    "            threshold: Seuil entre 0 et 1\n",
    "        \"\"\"\n",
    "        if 0 <= threshold <= 1:\n",
    "            self.threshold = threshold\n",
    "            logger.info(f\"Nouveau seuil de prédiction: {threshold}\")\n",
    "        else:\n",
    "            logger.warning(f\"Seuil invalide: {threshold}. Doit être entre 0 et 1.\")"
   ],
   "id": "68fbb56dac53b69f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Embedding de Compétences avec BERT",
   "id": "c9623cd7a815cc9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:12:03.350812Z",
     "start_time": "2025-06-04T12:12:03.222086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SkillEmbedder:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "        self.model.eval()\n",
    "        self.cache = {}\n",
    "\n",
    "    def embed_skill(self, skill_text):\n",
    "        if isinstance(skill_text, dict):\n",
    "            skill_text = str(skill_text)\n",
    "\n",
    "        if skill_text in self.cache:\n",
    "            return self.cache[skill_text]\n",
    "        try:\n",
    "            inputs = self.tokenizer(skill_text, return_tensors='pt', truncation=True, padding=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "            self.cache[skill_text] = embedding\n",
    "            return embedding\n",
    "        except:\n",
    "            return np.zeros(768)\n"
   ],
   "id": "205041be8c886498",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Company Classifier Implementation",
   "id": "cc561272cbe5e9b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:12:03.887015Z",
     "start_time": "2025-06-04T12:12:03.488832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "COMPANY_WEIGHTS = {\n",
    "    'large_company': 1.5,  # Augmenté de 1.2 à 1.5\n",
    "    'startup': 0.7,       # Réduit de 0.8 à 0.7\n",
    "    'default': 1.0\n",
    "}\n",
    "\n",
    "class CompanyClassifier(nn.Module):\n",
    "    \"\"\"Modèle complet de classification d'entreprises avec mécanisme d'attention\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=128, num_layers=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        # Couche d'embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Couche LSTM bidirectionnelle\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers>1 else 0\n",
    "        )\n",
    "\n",
    "        # Mécanisme d'attention\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim * 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim * 2, 1)\n",
    "        )\n",
    "\n",
    "        # Classificateur\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding des tokens\n",
    "        embedded = self.embedding_dropout(self.embedding(x))\n",
    "\n",
    "        # Passage dans le LSTM\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "\n",
    "        # Application de l'attention\n",
    "        attention_weights = F.softmax(self.attention(lstm_out), dim=1)\n",
    "        attn_out = torch.sum(attention_weights * lstm_out, dim=1)\n",
    "\n",
    "        # Classification finale\n",
    "        out = self.dropout(F.relu(self.fc1(attn_out)))\n",
    "        return torch.sigmoid(self.fc2(out)).squeeze(-1), attention_weights.squeeze(-1)\n",
    "\n",
    "# Définir la fonction tokenizer_func au niveau global\n",
    "def tokenizer_func(text):\n",
    "    \"\"\"Tokeniseur personnalisé pour les noms d'entreprises\"\"\"\n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "torch.serialization.add_safe_globals([tokenizer_func])\n",
    "\n",
    "class CompanyClassifierWrapper:\n",
    "    def __init__(self, model_path=None):\n",
    "        # Chemin réel du modèle\n",
    "        actual_model_path = r\"C:\\Users\\AzComputer\\PyCharmMiscProject\\models\\company_classifier.pth\"\n",
    "\n",
    "        # Déterminer le chemin à utiliser\n",
    "        if model_path is None:\n",
    "            model_path = actual_model_path\n",
    "        else:\n",
    "            # Si un chemin personnalisé est fourni, vérifier qu'il existe\n",
    "            if not os.path.exists(model_path):\n",
    "                logger.warning(f\"Le chemin spécifié {model_path} n'existe pas, utilisation du chemin par défaut\")\n",
    "                model_path = actual_model_path\n",
    "\n",
    "        logger.info(f\"Chargement du modèle depuis: {model_path}\")\n",
    "\n",
    "        try:\n",
    "            if not os.path.exists(model_path):\n",
    "                raise FileNotFoundError(f\"Fichier modèle introuvable: {model_path}. Veuillez vérifier que:\"\n",
    "                                     f\"\\n1. Le fichier existe à cet emplacement\"\n",
    "                                     f\"\\n2. Le chemin est correct\"\n",
    "                                     f\"\\n3. Vous avez les permissions de lecture\")\n",
    "\n",
    "            # Charger le modèle pré-entraîné\n",
    "            saved_data = torch.load(model_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "            # Ajuster les clés du state_dict si nécessaire\n",
    "            state_dict = saved_data['model_state_dict']\n",
    "            adjusted_state_dict = {}\n",
    "\n",
    "            for key, value in state_dict.items():\n",
    "                # Corriger les clés du mécanisme d'attention\n",
    "                if key.startswith('attention.attention.'):\n",
    "                    new_key = key.replace('attention.attention.', 'attention.')\n",
    "                    adjusted_state_dict[new_key] = value\n",
    "                    logger.debug(f\"Clé ajustée: {key} -> {new_key}\")\n",
    "                else:\n",
    "                    adjusted_state_dict[key] = value\n",
    "\n",
    "            self.model = CompanyClassifier(\n",
    "                vocab_size=len(saved_data['vocab']),\n",
    "                embedding_dim=128,\n",
    "                hidden_dim=128\n",
    "            )\n",
    "\n",
    "            # Charger le state_dict ajusté\n",
    "            load_result = self.model.load_state_dict(adjusted_state_dict, strict=False)\n",
    "            logger.info(f\"Résultat du chargement du modèle: {load_result}\")\n",
    "\n",
    "            self.model.eval()\n",
    "            self.vocab = saved_data['vocab']\n",
    "            self.max_length = saved_data['max_length']\n",
    "            self.tokenizer_func = tokenizer_func\n",
    "\n",
    "            logger.info(\"CompanyClassifier chargé avec succès\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du chargement du CompanyClassifier: {e}\")\n",
    "            self.model = None\n",
    "\n",
    "    def predict_company_size(self, company_name):\n",
    "        if self.model is None:\n",
    "            logger.warning(f\"Modèle non chargé, utilisation du poids par défaut pour: {company_name}\")\n",
    "            return COMPANY_WEIGHTS['default']\n",
    "\n",
    "        try:\n",
    "            # Tokenisation et conversion numérique\n",
    "            tokens = self.tokenizer_func(company_name)\n",
    "            numericalized = [self.vocab.get(token, self.vocab['<unk>']) for token in tokens]\n",
    "\n",
    "            if len(numericalized) > self.max_length:\n",
    "                numericalized = numericalized[:self.max_length]\n",
    "            else:\n",
    "                numericalized = numericalized + [self.vocab['<pad>']] * (self.max_length - len(numericalized))\n",
    "\n",
    "            # Conversion en tensor\n",
    "            input_tensor = torch.tensor([numericalized], dtype=torch.long)\n",
    "\n",
    "            # Prédiction\n",
    "            with torch.no_grad():\n",
    "                output, _ = self.model(input_tensor)\n",
    "                prob = output.item()\n",
    "\n",
    "            # Détermination du type d'entreprise\n",
    "            if prob > 0.7:  # Seuil élevé pour startup\n",
    "                company_type = 'startup'\n",
    "                weight = COMPANY_WEIGHTS['startup']\n",
    "            elif prob < 0.3:  # Seuil bas pour grande entreprise\n",
    "                company_type = 'large_company'\n",
    "                weight = COMPANY_WEIGHTS['large_company']\n",
    "            else:\n",
    "                company_type = 'default'\n",
    "                weight = COMPANY_WEIGHTS['default']\n",
    "\n",
    "            logger.info(f\"Entreprise: {company_name} | Type prédit: {company_type} (prob: {prob:.2f}) | Poids appliqué: {weight}\")\n",
    "            return weight\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de la prédiction pour {company_name}: {e}\")\n",
    "            return COMPANY_WEIGHTS['default']\n",
    "\n",
    "# Initialiser le wrapper avec logging détaillé\n",
    "logger.info(\"Initialisation du CompanyClassifierWrapper...\")\n",
    "company_classifier = CompanyClassifierWrapper()\n",
    "logger.info(\"CompanyClassifierWrapper initialisé\")"
   ],
   "id": "34151b5f5955a851",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initialisation du CompanyClassifierWrapper...\n",
      "INFO:__main__:Chargement du modèle depuis: C:\\Users\\AzComputer\\PyCharmMiscProject\\models\\company_classifier.pth\n",
      "INFO:__main__:Résultat du chargement du modèle: <All keys matched successfully>\n",
      "INFO:__main__:CompanyClassifier chargé avec succès\n",
      "INFO:__main__:CompanyClassifierWrapper initialisé\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. Calcul des Scores de Compétences",
   "id": "5633d2409bfc982d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:12:03.953077Z",
     "start_time": "2025-06-04T12:12:03.916032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def calculate_skill_scores(cvs_data):\n",
    "    \"\"\"Version optimisée et robuste qui gère tous les cas de valeurs None\"\"\"\n",
    "    from collections import defaultdict\n",
    "    import numpy as np\n",
    "\n",
    "    # 1. Initialisation et comptage des fréquences\n",
    "    global_skill_freq = defaultdict(int)\n",
    "    company_weights_cache = {}\n",
    "\n",
    "    # Compter les occurrences globales des compétences\n",
    "    for cv in cvs_data:\n",
    "        skills = cv.get('skills', []) or []  # Gestion du cas None\n",
    "        for skill in skills:\n",
    "            skill_name = skill.get('name', str(skill)) if isinstance(skill, dict) else str(skill)\n",
    "            global_skill_freq[skill_name] += 1\n",
    "\n",
    "    # 2. Nouvelle structure de données enrichie avec mentions GitHub\n",
    "    skill_info = defaultdict(lambda: {\n",
    "        'exp_mentions': 0,\n",
    "        'proj_mentions': 0,\n",
    "        'edu_mentions': 0,\n",
    "        'cert_mentions': 0,\n",
    "        'github_mentions': 0,  # Nouveau champ pour les mentions GitHub\n",
    "        'years_sum': 0,\n",
    "        'count': 0,\n",
    "        'company_weights': [],\n",
    "        'prestigious_companies': 0\n",
    "    })\n",
    "\n",
    "    # 3. Collecte complète des données avec gestion robuste des None\n",
    "    for cv in cvs_data:\n",
    "        skills = cv.get('skills', []) or []\n",
    "        years_exp = cv.get('years_of_experience', 0) or 0\n",
    "\n",
    "        # Pré-calcul des poids d'entreprise pour ce CV\n",
    "        exp_company_weights = []\n",
    "        for exp in cv.get('experiences', []) or []:\n",
    "            company_name = (exp.get('company', '') or '').strip()\n",
    "            if company_name:\n",
    "                if company_name not in company_weights_cache:\n",
    "                    weight = company_classifier.predict_company_size(company_name)\n",
    "                    company_weights_cache[company_name] = weight\n",
    "                    if weight > 1.0:\n",
    "                        skill_info['__prestigious_total'] = skill_info.get('__prestigious_total', 0) + 1\n",
    "                exp_company_weights.append(company_weights_cache[company_name])\n",
    "            else:\n",
    "                exp_company_weights.append(1.0)\n",
    "\n",
    "        # Analyse détaillée par compétence\n",
    "        for skill in skills:\n",
    "            skill_name = skill.get('name', str(skill)) if isinstance(skill, dict) else str(skill)\n",
    "            info = skill_info[skill_name]\n",
    "            info['count'] += 1\n",
    "            info['years_sum'] += years_exp\n",
    "\n",
    "            # Vérification des mentions dans les expériences\n",
    "            for i, exp in enumerate(cv.get('experiences', []) or []):\n",
    "                desc = (exp.get('description', '') or '').lower()\n",
    "                if skill_name.lower() in desc:\n",
    "                    info['exp_mentions'] += 1\n",
    "                    if i < len(exp_company_weights):\n",
    "                        weight = exp_company_weights[i]\n",
    "                        info['company_weights'].append(weight)\n",
    "                        if weight > 1.0:\n",
    "                            info['prestigious_companies'] += 1\n",
    "\n",
    "            # Vérification des mentions dans les projets\n",
    "            for proj in cv.get('projects', []) or []:\n",
    "                project_summary = (proj.get('project_summary', '') or '').lower()\n",
    "                if skill_name.lower() in project_summary:\n",
    "                    info['proj_mentions'] += 1\n",
    "\n",
    "            # Vérification des mentions dans l'éducation\n",
    "            for edu in cv.get('education', []) or []:\n",
    "                degree = (edu.get('degree', '') or '').lower()\n",
    "                institution = (edu.get('institution', '') or '').lower()\n",
    "                if skill_name.lower() in degree or skill_name.lower() in institution:\n",
    "                    info['edu_mentions'] += 1\n",
    "\n",
    "            # Vérification des mentions dans les certifications\n",
    "            for cert in cv.get('certifications', []) or []:\n",
    "                cert_str = (cert if isinstance(cert, str) else str(cert)) or ''\n",
    "                if skill_name.lower() in cert_str.lower():\n",
    "                    info['cert_mentions'] += 1\n",
    "\n",
    "            # Vérification des mentions dans les projets GitHub (nouveau)\n",
    "            for gh_proj in cv.get('github_projects', []) or []:\n",
    "                # Vérification dans la description\n",
    "                desc = (gh_proj.get('description', '') or '').lower()\n",
    "                if skill_name.lower() in desc:\n",
    "                    info['github_mentions'] += 1\n",
    "\n",
    "                # Vérification dans les langages utilisés\n",
    "                for lang in gh_proj.get('languages', []) or []:\n",
    "                    if skill_name.lower() == lang.lower():\n",
    "                        info['github_mentions'] += 1\n",
    "\n",
    "    # 4. Calcul des scores optimisés\n",
    "    total_prestigious = max(skill_info.get('__prestigious_total', 1), 1)\n",
    "    skill_scores = {}\n",
    "\n",
    "    for skill_name, info in skill_info.items():\n",
    "        if skill_name == '__prestigious_total':\n",
    "            continue\n",
    "\n",
    "        total = max(info['count'], 1)\n",
    "\n",
    "        # Calcul du company_score avec boost\n",
    "        if info['company_weights']:\n",
    "            max_weight = max(info['company_weights'])\n",
    "            company_score = 1 + (max_weight - 1) * 1.2\n",
    "        else:\n",
    "            company_score = 1.0\n",
    "\n",
    "        # Bonus de prestige\n",
    "        prestige_bonus = min(info['prestigious_companies'] / total_prestigious * 0.15, 0.15)\n",
    "\n",
    "        # Formule principale avec ajout des mentions GitHub (poids faible: 0.05)\n",
    "        base_score = (\n",
    "            0.45 * (info['exp_mentions'] / total) * company_score +\n",
    "            0.3 * (info['proj_mentions'] / total) +\n",
    "            0.15 * (info['edu_mentions'] / total) +\n",
    "            0.1 * (info['cert_mentions'] / total) +\n",
    "            0.05 * (info['github_mentions'] / total) +  # Nouveau terme GitHub\n",
    "            0.05 * min(info['years_sum'] / (total * 5), 1) +\n",
    "            prestige_bonus\n",
    "        )\n",
    "\n",
    "        # Ajustement de fréquence\n",
    "        freq_adj = 1 / (1 + np.log(1 + global_skill_freq[skill_name] / len(cvs_data)))**0.3\n",
    "        adj_score = min(base_score * freq_adj * 1.1, 0.99)\n",
    "\n",
    "        skill_scores[skill_name] = adj_score\n",
    "\n",
    "    # 5. Normalisation intelligente\n",
    "    if skill_scores:\n",
    "        scores = np.array(list(skill_scores.values()))\n",
    "        p25, p75 = np.percentile(scores, [25, 75])\n",
    "        iqr = max(p75 - p25, 1e-8)\n",
    "\n",
    "        for skill in skill_scores:\n",
    "            if iqr > 0:\n",
    "                normalized = 0.4 + 0.5 * ((skill_scores[skill] - p25) / iqr)\n",
    "            else:\n",
    "                normalized = 0.6\n",
    "\n",
    "            skill_scores[skill] = min(max(normalized, 0.3), 0.95)\n",
    "\n",
    "    return skill_scores"
   ],
   "id": "7eaaf9ebc237f96b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. Calcul de Similarité entre Compétences",
   "id": "da20a9c7fe3342f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:12:04.142043Z",
     "start_time": "2025-06-04T12:12:03.968094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_skill_similarity(skill_scores):\n",
    "    skills = list(skill_scores.keys())\n",
    "    embeddings = []\n",
    "\n",
    "    for skill in skills:\n",
    "        try:\n",
    "            embeddings.append(skill_embedder.embed_skill(skill))\n",
    "        except:\n",
    "            embeddings.append(np.zeros(768))\n",
    "\n",
    "    similarity_matrix = cosine_similarity(np.array(embeddings))\n",
    "\n",
    "    # Convertir en types Python natifs pour MongoDB\n",
    "    similarity_matrix = convert_numpy_types(similarity_matrix)\n",
    "\n",
    "    return skills, similarity_matrix\n"
   ],
   "id": "20b43e4c353df887",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 9. Calcul des Scores pour les CVs",
   "id": "2a9ddbe7e218ecb1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:12:04.334477Z",
     "start_time": "2025-06-04T12:12:04.265064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_cv_scores(cv_data, skill_scores, skill_similarity):\n",
    "    \"\"\"Version optimisée avec final_score = average_skill_score\"\"\"\n",
    "    skills, similarity_matrix = skill_similarity\n",
    "    cv_skills = cv_data.get('skills', [])\n",
    "    skill_scores_in_cv = []\n",
    "    max_possible_score = 0.95  # Plafond global conservé\n",
    "\n",
    "    # 1. Calcul des scores individuels avec boost (inchangé)\n",
    "    for skill in cv_skills:\n",
    "        skill_str = skill.get('name', str(skill)) if isinstance(skill, dict) else str(skill)\n",
    "\n",
    "        if skill_str in skill_scores:\n",
    "            base_score = skill_scores[skill_str]\n",
    "\n",
    "            # Boost de similarité\n",
    "            try:\n",
    "                idx = skills.index(skill_str)\n",
    "                similar_indices = [i for i in range(len(skills)) if similarity_matrix[idx][i] > 0.4]\n",
    "\n",
    "                if similar_indices:\n",
    "                    similar_scores = []\n",
    "                    for i in similar_indices:\n",
    "                        sim = similarity_matrix[idx][i]\n",
    "                        weight = sim ** 1.5\n",
    "                        similar_scores.append((skill_scores[skills[i]], weight))\n",
    "\n",
    "                    total_weight = sum(w for _, w in similar_scores)\n",
    "                    if total_weight > 0:\n",
    "                        weighted_avg = sum(s * w for s, w in similar_scores) / total_weight\n",
    "                        boosted_score = min(base_score * 0.6 + weighted_avg * 0.4, max_possible_score)\n",
    "                    else:\n",
    "                        boosted_score = base_score\n",
    "                else:\n",
    "                    boosted_score = base_score\n",
    "            except:\n",
    "                boosted_score = base_score\n",
    "\n",
    "            # Bonus pour compétences principales\n",
    "            if boosted_score > 0.7:\n",
    "                boosted_score = min(boosted_score * 1.05, max_possible_score)\n",
    "\n",
    "            skill_scores_in_cv.append(boosted_score)\n",
    "\n",
    "    # 2. Calcul de la moyenne pondérée (conservée mais résultat utilisé différemment)\n",
    "    if skill_scores_in_cv:\n",
    "        weights = [s**3 for s in skill_scores_in_cv]\n",
    "        avg_skill_score = np.average(skill_scores_in_cv, weights=weights)\n",
    "    else:\n",
    "        avg_skill_score = 0\n",
    "\n",
    "    # 3. Formattage des résultats avec final_score = average_skill_score\n",
    "    return {\n",
    "        'skill_scores': {s.get('name', str(s)) if isinstance(s, dict) else str(s): float(score)\n",
    "                        for s, score in zip(cv_skills, skill_scores_in_cv)},\n",
    "        'average_skill_score': float(avg_skill_score),\n",
    "        'final_score': float(avg_skill_score)  # Ici on égalise explicitement les deux valeurs\n",
    "    }"
   ],
   "id": "acdc5ad1d4aa14a5",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 10. Similarité entre CVs par Catégorie",
   "id": "1a3011c7bcb5e08"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:12:04.547271Z",
     "start_time": "2025-06-04T12:12:04.409473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_cv_similarity_by_category(cvs_collection):\n",
    "    \"\"\"Calcule la similarité entre CVs de la même catégorie en utilisant seulement les CVs de référence\"\"\"\n",
    "    # Récupérer toutes les catégories distinctes (en considérant que chaque CV peut avoir plusieurs catégories)\n",
    "    all_categories = set()\n",
    "    for cv in cvs_collection.find({}, {\"categories\": 1}):\n",
    "        all_categories.update(cv.get(\"categories\", []))\n",
    "\n",
    "    category_results = {}\n",
    "\n",
    "    for category in all_categories:\n",
    "        # Récupérer tous les CVs qui ont cette catégorie avec leurs scores\n",
    "        cvs_in_category = list(cvs_collection.find({\"categories\": category},\n",
    "                                                  {\"name\": 1, \"final_score\": 1, \"_id\": 1, \"categories\": 1}))\n",
    "\n",
    "        if len(cvs_in_category) < 2:\n",
    "            logger.info(f\"Catégorie {category} n'a qu'un seul CV, pas de similarité à calculer\")\n",
    "            continue\n",
    "\n",
    "        # Trier les CVs par score\n",
    "        cvs_sorted = sorted(cvs_in_category, key=lambda x: x.get('final_score', 0))\n",
    "\n",
    "        # Sélectionner les CVs de référence\n",
    "        reference_cvs = [\n",
    "            cvs_sorted[0],  # Score min\n",
    "            cvs_sorted[-1],  # Score max\n",
    "        ]\n",
    "\n",
    "        # Ajouter deux CVs médians si possible\n",
    "        if len(cvs_sorted) > 2:\n",
    "            mid = len(cvs_sorted) // 2\n",
    "            reference_cvs.append(cvs_sorted[mid])\n",
    "            if len(cvs_sorted) > 3:\n",
    "                reference_cvs.append(cvs_sorted[mid-1] if mid > 1 else cvs_sorted[1])\n",
    "\n",
    "        # Charger les données complètes des CVs de référence\n",
    "        reference_cvs_full = list(cvs_collection.find(\n",
    "            {\"_id\": {\"$in\": [cv[\"_id\"] for cv in reference_cvs]}}\n",
    "        ))\n",
    "\n",
    "        # Préparer les embeddings pour chaque CV de référence\n",
    "        cv_texts = []\n",
    "        cv_embeddings = []\n",
    "        reference_names = []\n",
    "\n",
    "        for cv in reference_cvs_full:\n",
    "            try:\n",
    "                # Convertir le CV en texte\n",
    "                text = category_predictor._cv_to_text(cv)\n",
    "                cv_texts.append(text)\n",
    "                reference_names.append(cv.get('name', 'Inconnu'))\n",
    "\n",
    "                # Générer l'embedding avec BERT\n",
    "                inputs = skill_embedder.tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "                with torch.no_grad():\n",
    "                    outputs = skill_embedder.model(**inputs)\n",
    "                embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "                cv_embeddings.append(embedding)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Erreur lors du traitement du CV {cv.get('name', 'Inconnu')}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if len(cv_embeddings) < 2:\n",
    "            logger.info(f\"Catégorie {category} n'a pas assez de CVs de référence valides pour calculer la similarité\")\n",
    "            continue\n",
    "\n",
    "        # Calculer la matrice de similarité entre les CVs de référence\n",
    "        similarity_matrix = cosine_similarity(np.array(cv_embeddings))\n",
    "\n",
    "        # Stocker les résultats\n",
    "        category_results[category] = {\n",
    "            'reference_cvs': reference_names,\n",
    "            'reference_scores': [cv.get('final_score', 0) for cv in reference_cvs_full],\n",
    "            'similarity_matrix': convert_numpy_types(similarity_matrix),\n",
    "            'average_similarity': float(np.mean(similarity_matrix))\n",
    "        }\n",
    "\n",
    "        logger.info(f\"Similarité calculée pour la catégorie {category} avec {len(reference_cvs_full)} CVs de référence, similarité moyenne: {category_results[category]['average_similarity']:.2f}\")\n",
    "\n",
    "    return category_results"
   ],
   "id": "9cdcb78b3042a44a",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 11. Mise à Jour des CVs dans MongoDB",
   "id": "5467c6e059262997"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:12:04.729885Z",
     "start_time": "2025-06-04T12:12:04.602273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_resumes_in_mongodb(cvs_collection, cvs_data, skill_scores, skill_similarity):\n",
    "    category_predictor = CVCategoryPredictor()\n",
    "    update_results = []\n",
    "\n",
    "    for cv in cvs_data:\n",
    "        try:\n",
    "            # Calcul des scores pour ce CV\n",
    "            scores = calculate_cv_scores(cv, skill_scores, skill_similarity)\n",
    "\n",
    "            # Conversion des types numpy\n",
    "            scores = convert_numpy_types(scores)\n",
    "\n",
    "            # Prédiction de la catégorie (peut retourner plusieurs catégories maintenant)\n",
    "            categories = category_predictor.predict_category(cv)\n",
    "\n",
    "            # Préparation des compétences avec scores\n",
    "            skills_with_scores = []\n",
    "            existing_skills = cv.get('skills', [])\n",
    "\n",
    "            for skill in existing_skills:\n",
    "                skill_name = skill.get('name', str(skill)) if isinstance(skill, dict) else str(skill)\n",
    "\n",
    "                # Création du nouvel objet skill avec le score\n",
    "                updated_skill = {\n",
    "                    'name': skill_name,\n",
    "                    'score': scores['skill_scores'].get(skill_name, 0.0)\n",
    "                }\n",
    "\n",
    "                # Si la compétence originale était un dictionnaire, conserver tous les champs existants\n",
    "                if isinstance(skill, dict):\n",
    "                    updated_skill.update({k: v for k, v in skill.items() if k != 'score'})\n",
    "\n",
    "                skills_with_scores.append(updated_skill)\n",
    "\n",
    "            # Préparation de la mise à jour\n",
    "            update_data = {\n",
    "                'categories': categories,  # Maintenant une liste de catégories\n",
    "                'skills': skills_with_scores,\n",
    "                'average_skill_score': scores['average_skill_score'],\n",
    "                'final_score': scores['final_score'],\n",
    "                'last_updated': datetime.now()\n",
    "            }\n",
    "\n",
    "            # Mise à jour dans MongoDB\n",
    "            result = cvs_collection.update_one(\n",
    "                {'_id': cv['_id']},\n",
    "                {'$set': update_data},\n",
    "                upsert=False\n",
    "            )\n",
    "\n",
    "            if result.modified_count == 1:\n",
    "                logger.info(f\"CV {cv.get('name', 'Inconnu')} mis à jour avec succès\")\n",
    "            else:\n",
    "                logger.warning(f\"Aucune modification pour le CV {cv.get('name', 'Inconnu')}\")\n",
    "\n",
    "            update_results.append(result)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de la mise à jour du CV {cv.get('name', 'Inconnu')}: {str(e)}\")\n",
    "\n",
    "    return update_results"
   ],
   "id": "6624933611e2867f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 12. Exécution Principale",
   "id": "685ef6e91ebf8e0a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:15:32.034865Z",
     "start_time": "2025-06-04T12:12:04.745907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialisation des composants\n",
    "skill_embedder = SkillEmbedder()\n",
    "category_predictor = CVCategoryPredictor()\n",
    "\n",
    "# Calcul des scores de compétences\n",
    "skill_scores = calculate_skill_scores(cvs_data)\n",
    "skill_scores = convert_numpy_types(skill_scores)\n",
    "\n",
    "# Calcul de la similarité entre compétences\n",
    "skill_similarity = compute_skill_similarity(skill_scores)\n",
    "\n",
    "# Mise à jour des CVs dans MongoDB\n",
    "update_results = update_resumes_in_mongodb(cvs_collection, cvs_data, skill_scores, skill_similarity)\n",
    "\n",
    "# Calcul de la similarité entre CVs par catégorie\n",
    "cv_similarity_by_category = calculate_cv_similarity_by_category(cvs_collection)\n"
   ],
   "id": "6bf0a23a386dd716",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Binarizer chargé. Classes disponibles: ['ACCOUNTANT' 'ADVOCATE' 'AGRICULTURE' 'AI Specialist' 'API Developer'\n",
      " 'APPAREL' 'ARTS' 'AUTOMOBILE' 'AVIATION' 'Advocate'\n",
      " 'Analytics Professional' 'Angular Developer' 'Arts' 'Automation Testing'\n",
      " 'BANKING' 'BPO' 'BUSINESS-DEVELOPMENT' 'Backend Developer'\n",
      " 'Big Data Engineer' 'Blockchain' 'Blockchain Developer'\n",
      " 'Business Analyst' 'CHEF' 'CI/CD Specialist' 'CONSTRUCTION' 'CONSULTANT'\n",
      " 'Civil Engineer' 'Cloud Engineer' 'Cloud Practitioner'\n",
      " 'Computer Vision Specialist' 'Containerization Expert' 'DESIGNER'\n",
      " 'DIGITAL-MEDIA' 'Data Science' 'Data Scientist' 'Database'\n",
      " 'Deep Learning Engineer' 'DevOps Engineer' 'Django Developer'\n",
      " 'DotNet Developer' 'ENGINEERING' 'ETL Developer' 'Electrical Engineering'\n",
      " 'FINANCE' 'FITNESS' 'Flask Developer' 'Frontend Developer'\n",
      " 'Full Stack Developer' 'HEALTHCARE' 'HR' 'Hadoop' 'Health and fitness'\n",
      " 'INFORMATION-TECHNOLOGY' 'IoT Developer' 'Java Developer'\n",
      " 'JavaScript Developer' 'ML Engineer' 'Machine Learning Engineer'\n",
      " 'Mechanical Engineer' 'Mobile App Developer (iOS/Android)' 'NLP Engineer'\n",
      " 'Network Security Engineer' 'NoSQL Specialist' 'Node.js Developer'\n",
      " 'Operations Manager' 'PMO' 'PUBLIC-RELATIONS' 'Python Developer'\n",
      " 'React Specialist' 'SALES' 'SAP Developer' 'SQL Expert' 'Sales'\n",
      " 'Security Specialist' 'Server-side Developer' 'System Administrator'\n",
      " 'TEACHER' 'Testing' 'TypeScript Developer' 'UI Developer'\n",
      " 'Vue.js Developer' 'Web Designing' 'Web Developer']\n",
      "INFO:__main__:Chargement du modèle depuis C:\\Users\\AzComputer\\PyCharmMiscProject\\models\\mon_modele_bert_multilabel\n",
      "INFO:__main__:Modèle principal chargé avec succès\n",
      "INFO:__main__:Entreprise: Microsoft | Type prédit: large_company (prob: 0.11) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: DSO National Laboratories | Type prédit: large_company (prob: 0.07) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: N/A | Type prédit: large_company (prob: 0.23) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: Google | Type prédit: large_company (prob: 0.23) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: NUS School of Computing | Type prédit: large_company (prob: 0.02) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: IDA | Type prédit: large_company (prob: 0.23) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: PACIS | Type prédit: large_company (prob: 0.23) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: HP Inc. | Type prédit: default (prob: 0.63) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: DDB Worldwide Pte Ltd | Type prédit: large_company (prob: 0.25) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: College Info Geek (collegeinfogeek.com) | Type prédit: default (prob: 0.44) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: Iowa State University | Type prédit: large_company (prob: 0.25) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: Genisup India Pvt. Ltd. | Type prédit: large_company (prob: 0.11) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: VUGS Technologies Pvt. Ltd. | Type prédit: default (prob: 0.61) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: Verisk - Wood Mackenzie (Hyderabad, India) | Type prédit: large_company (prob: 0.28) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: Supply Chain Data & Analytics | Type prédit: default (prob: 0.34) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: REO PRODUCTIONS | Type prédit: default (prob: 0.31) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: LEAN ORGANISATION | Type prédit: default (prob: 0.31) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: FURBANS TECHNOLOGIES | Type prédit: default (prob: 0.62) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: Digital Nirvana Inc. | Type prédit: default (prob: 0.64) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: AlmaBetter | Type prédit: large_company (prob: 0.23) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: Flip Robo Technologies | Type prédit: default (prob: 0.60) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: Team Lease Company | Type prédit: large_company (prob: 0.10) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: Almabetter | Type prédit: large_company (prob: 0.23) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: Sony Research India Private Limited | Type prédit: large_company (prob: 0.02) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: upGrad | Type prédit: large_company (prob: 0.23) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: Bajaj Capital Limited, Gurugram | Type prédit: large_company (prob: 0.02) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: Infosys | Type prédit: large_company (prob: 0.08) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: Legato Healthcare | Type prédit: large_company (prob: 0.11) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: Infosys Ltd | Type prédit: large_company (prob: 0.12) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: IBM | Type prédit: large_company (prob: 0.18) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: Anarock | Type prédit: large_company (prob: 0.23) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: Lentra | Type prédit: large_company (prob: 0.23) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: Tanalink | Type prédit: large_company (prob: 0.23) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: AirAsia Digital | Type prédit: default (prob: 0.58) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: Universiti Teknologi PETRONAS | Type prédit: default (prob: 0.35) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: Petrolink Services | Type prédit: large_company (prob: 0.04) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: Schneider Electric | Type prédit: large_company (prob: 0.01) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: Tech Mahindra Limited | Type prédit: large_company (prob: 0.01) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: Siemens Technology and Services Private Limited | Type prédit: large_company (prob: 0.01) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: Robert Bosch Engineering and Business Solutions Private Limited | Type prédit: large_company (prob: 0.02) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: HCL Technologies Ltd | Type prédit: default (prob: 0.60) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: Infosys Limited, Chandigarh | Type prédit: large_company (prob: 0.08) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: EXL Services, Gurgaon | Type prédit: large_company (prob: 0.07) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: Sabudh Foundation | Type prédit: default (prob: 0.31) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: Chitra Security Services | Type prédit: large_company (prob: 0.07) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: HCL Technologies | Type prédit: default (prob: 0.60) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: FCS Software Solutions | Type prédit: default (prob: 0.64) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: Spice Digital | Type prédit: default (prob: 0.58) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: Digz Placements | Type prédit: default (prob: 0.31) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: DSP Precision Products Pvt. Ltd. | Type prédit: large_company (prob: 0.18) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: AB InBv | Type prédit: large_company (prob: 0.04) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: Optum Global Solutions(UHG) | Type prédit: large_company (prob: 0.07) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: Innominds | Type prédit: large_company (prob: 0.23) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: OSI Digital | Type prédit: default (prob: 0.58) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: Rubixe | Type prédit: large_company (prob: 0.23) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: Duracool aircondtioning | Type prédit: default (prob: 0.31) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: Rao and co HVAC sales and services pvt ltd | Type prédit: large_company (prob: 0.04) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: Daily Code Solutions | Type prédit: default (prob: 0.31) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: Alma Better | Type prédit: default (prob: 0.31) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: Chat-Bot NLP Model on Hugging Face | Type prédit: default (prob: 0.39) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: Mehran UET, Jamshoro | Type prédit: default (prob: 0.35) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: Prince Sultan University | Type prédit: default (prob: 0.35) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: National Institute of Electronics, ISB | Type prédit: large_company (prob: 0.01) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: SEECS, NUST | Type prédit: default (prob: 0.31) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: TechCorp | Type prédit: large_company (prob: 0.23) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: Luna Web Design | Type prédit: default (prob: 0.45) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: GHT Company | Type prédit: large_company (prob: 0.05) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: Startup Corporation | Type prédit: default (prob: 0.60) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: LUXURY CAR CENTER | Type prédit: default (prob: 0.35) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: The Wesley Center | Type prédit: default (prob: 0.40) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: Rainbow Special Care Center | Type prédit: large_company (prob: 0.12) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: Tech Solutions | Type prédit: large_company (prob: 0.08) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: DataCorp | Type prédit: large_company (prob: 0.23) | Poids appliqué: 1.5\n",
      "INFO:__main__:Entreprise: AI Innovations | Type prédit: default (prob: 0.31) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: Data Insights | Type prédit: default (prob: 0.46) | Poids appliqué: 1.0\n",
      "INFO:__main__:Entreprise: Cowell Elementary | Type prédit: default (prob: 0.31) | Poids appliqué: 1.0\n",
      "INFO:__main__:Binarizer chargé. Classes disponibles: ['ACCOUNTANT' 'ADVOCATE' 'AGRICULTURE' 'AI Specialist' 'API Developer'\n",
      " 'APPAREL' 'ARTS' 'AUTOMOBILE' 'AVIATION' 'Advocate'\n",
      " 'Analytics Professional' 'Angular Developer' 'Arts' 'Automation Testing'\n",
      " 'BANKING' 'BPO' 'BUSINESS-DEVELOPMENT' 'Backend Developer'\n",
      " 'Big Data Engineer' 'Blockchain' 'Blockchain Developer'\n",
      " 'Business Analyst' 'CHEF' 'CI/CD Specialist' 'CONSTRUCTION' 'CONSULTANT'\n",
      " 'Civil Engineer' 'Cloud Engineer' 'Cloud Practitioner'\n",
      " 'Computer Vision Specialist' 'Containerization Expert' 'DESIGNER'\n",
      " 'DIGITAL-MEDIA' 'Data Science' 'Data Scientist' 'Database'\n",
      " 'Deep Learning Engineer' 'DevOps Engineer' 'Django Developer'\n",
      " 'DotNet Developer' 'ENGINEERING' 'ETL Developer' 'Electrical Engineering'\n",
      " 'FINANCE' 'FITNESS' 'Flask Developer' 'Frontend Developer'\n",
      " 'Full Stack Developer' 'HEALTHCARE' 'HR' 'Hadoop' 'Health and fitness'\n",
      " 'INFORMATION-TECHNOLOGY' 'IoT Developer' 'Java Developer'\n",
      " 'JavaScript Developer' 'ML Engineer' 'Machine Learning Engineer'\n",
      " 'Mechanical Engineer' 'Mobile App Developer (iOS/Android)' 'NLP Engineer'\n",
      " 'Network Security Engineer' 'NoSQL Specialist' 'Node.js Developer'\n",
      " 'Operations Manager' 'PMO' 'PUBLIC-RELATIONS' 'Python Developer'\n",
      " 'React Specialist' 'SALES' 'SAP Developer' 'SQL Expert' 'Sales'\n",
      " 'Security Specialist' 'Server-side Developer' 'System Administrator'\n",
      " 'TEACHER' 'Testing' 'TypeScript Developer' 'UI Developer'\n",
      " 'Vue.js Developer' 'Web Designing' 'Web Developer']\n",
      "INFO:__main__:Chargement du modèle depuis C:\\Users\\AzComputer\\PyCharmMiscProject\\models\\mon_modele_bert_multilabel\n",
      "INFO:__main__:Modèle principal chargé avec succès\n",
      "INFO:__main__:Catégories prédites: ['UNKNOWN']\n",
      "INFO:__main__:CV Alice Clark mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['Cloud Practitioner', 'JavaScript Developer', 'Python Developer', 'SQL Expert']\n",
      "INFO:__main__:CV Chia Yong Kang mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['UNKNOWN']\n",
      "INFO:__main__:CV Michael Smith mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['UNKNOWN']\n",
      "INFO:__main__:CV xinni chng mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['Cloud Practitioner', 'JavaScript Developer', 'ML Engineer', 'Python Developer', 'SQL Expert']\n",
      "INFO:__main__:CV Ashly Lau mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['JavaScript Developer', 'SQL Expert']\n",
      "INFO:__main__:CV Thomas Frank mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['Cloud Practitioner', 'JavaScript Developer', 'ML Engineer', 'Python Developer', 'SQL Expert']\n",
      "INFO:__main__:CV ANUVA GOYAL mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['Big Data Engineer', 'Cloud Practitioner', 'ML Engineer', 'NLP Engineer', 'Python Developer', 'SQL Expert']\n",
      "INFO:__main__:CV ANUVA GOYAL mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['Cloud Practitioner', 'ML Engineer', 'Python Developer', 'SQL Expert']\n",
      "INFO:__main__:CV TIMTIM RAHMAN mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['Python Developer']\n",
      "INFO:__main__:CV ASWATHI VIJAYAN mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['Big Data Engineer', 'Cloud Practitioner', 'ML Engineer', 'NLP Engineer', 'Python Developer', 'SQL Expert']\n",
      "INFO:__main__:CV Varsha Barshab Nayak mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['Cloud Practitioner', 'ML Engineer', 'Python Developer', 'SQL Expert']\n",
      "INFO:__main__:CV Fulkar Khan mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['Big Data Engineer', 'Cloud Practitioner', 'ML Engineer', 'NLP Engineer', 'Python Developer', 'SQL Expert']\n",
      "INFO:__main__:CV AFTENDRA JHA mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['Cloud Practitioner', 'ML Engineer', 'Python Developer', 'SQL Expert']\n",
      "INFO:__main__:CV Kunika Bhargav mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['UNKNOWN']\n",
      "INFO:__main__:CV Kyoti Khan mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['Big Data Engineer', 'Cloud Practitioner', 'ML Engineer', 'Python Developer', 'SQL Expert']\n",
      "INFO:__main__:CV Toshan Tile mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['Big Data Engineer', 'Cloud Practitioner', 'ML Engineer', 'Python Developer', 'SQL Expert']\n",
      "INFO:__main__:CV Siddhi Shukla mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['Big Data Engineer', 'Cloud Practitioner', 'ML Engineer', 'NLP Engineer', 'Python Developer']\n",
      "INFO:__main__:CV ANMISHA MURARKA mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['Cloud Practitioner', 'ML Engineer', 'Python Developer']\n",
      "INFO:__main__:CV Aviw.upes01 mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['Big Data Engineer', 'Cloud Practitioner', 'ML Engineer', 'Python Developer']\n",
      "INFO:__main__:CV ABC KUMAR SINHA mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['Big Data Engineer', 'Cloud Practitioner', 'ML Engineer', 'Python Developer']\n",
      "INFO:__main__:CV Surili Chawla mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['Big Data Engineer', 'Cloud Practitioner', 'ML Engineer', 'Python Developer']\n",
      "INFO:__main__:CV Farukh Sharma mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['Big Data Engineer', 'Cloud Practitioner', 'Deep Learning Engineer', 'ML Engineer', 'NLP Engineer', 'Python Developer', 'SQL Expert']\n",
      "INFO:__main__:CV A mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['Big Data Engineer', 'Cloud Practitioner', 'ML Engineer', 'NLP Engineer', 'Python Developer']\n",
      "INFO:__main__:CV SAGAR KURUVA mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['Big Data Engineer', 'Cloud Practitioner', 'ML Engineer', 'NLP Engineer', 'Python Developer', 'SQL Expert']\n",
      "INFO:__main__:CV KANOHARAPV mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['Cloud Practitioner', 'ML Engineer', 'Python Developer', 'SQL Expert']\n",
      "INFO:__main__:CV  mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['Big Data Engineer', 'Cloud Practitioner', 'JavaScript Developer', 'ML Engineer', 'Python Developer', 'SQL Expert']\n",
      "INFO:__main__:CV Dr. Azam Rafique mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['UNKNOWN']\n",
      "INFO:__main__:CV Dhafer J Almakhles mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['Big Data Engineer', 'Cloud Practitioner', 'ML Engineer', 'Python Developer']\n",
      "INFO:__main__:CV Dr. Amjad Ali mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['ML Engineer', 'Python Developer', 'SQL Expert']\n",
      "INFO:__main__:CV NOUMAN ALI mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['ML Engineer', 'Python Developer']\n",
      "INFO:__main__:CV Shahid Mumtaz mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['JavaScript Developer', 'SQL Expert']\n",
      "INFO:__main__:CV Christoper M mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['SQL Expert']\n",
      "INFO:__main__:CV Christopher Morgan mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['JavaScript Developer', 'SQL Expert']\n",
      "INFO:__main__:CV Christopher Summary mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['JavaScript Developer', 'SQL Expert']\n",
      "INFO:__main__:CV Christopher Morgan mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['UNKNOWN']\n",
      "INFO:__main__:CV Elizabeth Holmes mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['SQL Expert']\n",
      "INFO:__main__:CV Christopher Morgan mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['UNKNOWN']\n",
      "INFO:__main__:CV Elizabeth Holmes mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['UNKNOWN']\n",
      "INFO:__main__:CV John W. Smith mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['Cloud Practitioner', 'ML Engineer', 'Python Developer', 'SQL Expert']\n",
      "INFO:__main__:CV James Carter mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['JavaScript Developer', 'SQL Expert']\n",
      "INFO:__main__:CV Christopher Morgan mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['Big Data Engineer', 'Cloud Practitioner', 'Deep Learning Engineer', 'ML Engineer', 'Machine Learning Engineer', 'NLP Engineer', 'Python Developer', 'SQL Expert']\n",
      "INFO:__main__:CV Sophia Williams mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['TEACHER']\n",
      "INFO:__main__:CV John W. Smith mis à jour avec succès\n",
      "INFO:__main__:Catégories prédites: ['JavaScript Developer', 'SQL Expert']\n",
      "INFO:__main__:CV CHRISTOPER mis à jour avec succès\n",
      "INFO:__main__:Similarité calculée pour la catégorie Deep Learning Engineer avec 2 CVs de référence, similarité moyenne: 0.99\n",
      "INFO:__main__:Similarité calculée pour la catégorie UNKNOWN avec 4 CVs de référence, similarité moyenne: 0.86\n",
      "INFO:__main__:Similarité calculée pour la catégorie JavaScript Developer avec 4 CVs de référence, similarité moyenne: 0.97\n",
      "INFO:__main__:Similarité calculée pour la catégorie ML Engineer avec 4 CVs de référence, similarité moyenne: 0.97\n",
      "INFO:__main__:Similarité calculée pour la catégorie SQL Expert avec 4 CVs de référence, similarité moyenne: 0.97\n",
      "INFO:__main__:Similarité calculée pour la catégorie NLP Engineer avec 4 CVs de référence, similarité moyenne: 0.98\n",
      "INFO:__main__:Similarité calculée pour la catégorie Cloud Practitioner avec 4 CVs de référence, similarité moyenne: 0.98\n",
      "INFO:__main__:Similarité calculée pour la catégorie Python Developer avec 4 CVs de référence, similarité moyenne: 0.98\n",
      "INFO:__main__:Similarité calculée pour la catégorie Big Data Engineer avec 4 CVs de référence, similarité moyenne: 0.97\n",
      "INFO:__main__:Catégorie TEACHER n'a qu'un seul CV, pas de similarité à calculer\n",
      "INFO:__main__:Catégorie Machine Learning Engineer n'a qu'un seul CV, pas de similarité à calculer\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 13. Résultats et Export",
   "id": "edb3d03a457f96f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:15:33.035406Z",
     "start_time": "2025-06-04T12:15:32.140926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Statistiques des mises à jour\n",
    "success_count = sum(1 for r in update_results if r is not None and r.matched_count > 0)\n",
    "logger.info(f\"Mise à jour terminée: {success_count}/{len(cvs_data)} CVs mis à jour avec succès\")\n",
    "\n",
    "# Export des résultats\n",
    "try:\n",
    "    # Export JSON\n",
    "    with open('../cv_scores_results.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump({\n",
    "            'skill_scores': skill_scores,\n",
    "            'cv_similarity_by_category': cv_similarity_by_category,\n",
    "            'update_count': success_count,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "    # Export CSV des scores\n",
    "    scores_df = pd.DataFrame([\n",
    "        {\n",
    "            'name': cv.get('name', 'Inconnu'),\n",
    "            'category': cv.get('category', 'UNKNOWN'),\n",
    "            'final_score': cv.get('final_score', 0),\n",
    "            'skills_count': len(cv.get('skills', [])),\n",
    "            'last_updated': cv.get('last_updated', datetime.now())\n",
    "        }\n",
    "        for cv in cvs_collection.find({}, {'name': 1, 'category': 1, 'final_score': 1, 'skills': 1, 'last_updated': 1})\n",
    "    ])\n",
    "    scores_df.to_csv('cv_scores_results.csv', index=False, encoding='utf-8')\n",
    "\n",
    "    logger.info(\"Export des résultats terminé avec succès\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erreur lors de l'export des résultats: {e}\")"
   ],
   "id": "91650ccae54cc1b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Mise à jour terminée: 44/44 CVs mis à jour avec succès\n",
      "INFO:__main__:Export des résultats terminé avec succès\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
